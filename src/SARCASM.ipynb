{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from data_processing import get_clean_data  # 导入预处理函数\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from models import build_cnn_model, build_lstm_model  # 从 models.py 中导入模型构建函数\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据并保存到本地\n",
    "train_cleaned, train_labels, test_cleaned, test_labels = get_clean_data(\n",
    "    \"../data/datasets/ghosh/train_sample.txt\",\n",
    "    \"../data/datasets/ghosh/test_sample.txt\",\n",
    "    save=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_tweets_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_lines = f.read().splitlines()\n",
    "\n",
    "with open(\"test_tweets_clean.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_lines = f.read().splitlines()\n",
    "\n",
    "train_cleaned_df = pd.DataFrame(train_lines, columns=[\"Text\"])\n",
    "test_cleaned_df = pd.DataFrame(test_lines, columns=[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以添加高频词（在词向量中的展示可以展示高频词词向量），可添加可视化预处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_cleaned_df[\"Text\"].apply(lambda x: x.split()).tolist()\n",
    "test_sentences = test_cleaned_df[\"Text\"].apply(lambda x: x.split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_model = Word2Vec(sentences=train_sentences, vector_size=100, window=5, sg=1, min_count=1, workers=4)\n",
    "skipgram_model.save(\"skipgram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = FastText(sentences=train_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "fasttext_model.save(\"fasttext.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示部分词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram\n",
      "love: [ 0.0033984   0.00861393  0.00138705  0.0082688  -0.00935446]\n",
      "my: [ 0.0053937   0.00297525 -0.01076279  0.00685094 -0.00698826]\n",
      "mom: [-0.00038277  0.00436646  0.0007396   0.003766   -0.00822128]\n",
      "like: [-0.0041687  -0.00663539  0.00905645  0.00450114 -0.00133147]\n",
      "just: [-0.00535828 -0.00484555 -0.00930921 -0.00540644  0.00328046]\n"
     ]
    }
   ],
   "source": [
    "# 选取几个单词展示词向量\n",
    "words = [\"love\", \"my\", \"mom\", \"like\", \"just\"]\n",
    "\n",
    "print(\"Skip-gram\")\n",
    "for word in words:\n",
    "    if word in skipgram_model.wv:\n",
    "        print(f\"{word}: {skipgram_model.wv[word][:5]}\")  # 仅展示前5个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText\n",
      "love: [-0.00380938  0.00047163 -0.00141727  0.00353753  0.00149987]\n",
      "my: [ 0.00290381  0.00189729  0.001617    0.00323172 -0.00172936]\n",
      "mom: [-0.00123451 -0.00050335  0.00012947  0.00161764  0.00149152]\n",
      "like: [ 1.4855972e-03 -4.1661761e-03  2.8692514e-03  4.2900465e-05\n",
      " -2.5856479e-03]\n",
      "just: [ 0.00101466 -0.00263574  0.00162182 -0.00044807 -0.0011162 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"FastText\")\n",
    "for word in words:\n",
    "    if word in fasttext_model.wv:\n",
    "        print(f\"{word}: {fasttext_model.wv[word][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_cleaned_df[\"Text\"].tolist())\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 词汇表大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_cleaned_df[\"Text\"].tolist())\n",
    "X_test = tokenizer.texts_to_sequences(test_cleaned_df[\"Text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths = [len(seq) for seq in X_train]\n",
    "max_len = int(np.percentile(train_lengths, 95))\n",
    "if max_len < 10:\n",
    "    max_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造嵌入矩阵函数：将 tokenizer 中的词汇与词向量模型对齐\n",
    "def create_embedding_matrix(model, tokenizer, vocab_size, embedding_dim):\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in model.wv:\n",
    "            embedding_matrix[i] = model.wv[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # 词向量维度（与训练时保持一致）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_embedding_matrix = create_embedding_matrix(skipgram_model, tokenizer, vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedding_matrix = create_embedding_matrix(fasttext_model, tokenizer, vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18052\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cnn_skipgram = build_cnn_model(skipgram_embedding_matrix, vocab_size, max_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_fasttext = build_cnn_model(fasttext_embedding_matrix, vocab_size, max_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_skipgram = build_lstm_model(skipgram_embedding_matrix, vocab_size, max_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_fasttext = build_lstm_model(fasttext_embedding_matrix, vocab_size, max_len, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.4112 - loss: 0.6937 - val_accuracy: 0.4500 - val_loss: 0.6953\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6252 - loss: 0.6884 - val_accuracy: 0.4500 - val_loss: 0.6960\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5738 - loss: 0.6851 - val_accuracy: 0.4500 - val_loss: 0.6983\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5665 - loss: 0.6789 - val_accuracy: 0.4500 - val_loss: 0.7048\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5500 - loss: 0.6736 - val_accuracy: 0.4500 - val_loss: 0.7139\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "# 训练 CNN 模型（使用 skip-gram 词向量）\n",
    "history_cnn = cnn_skipgram.fit(X_train, train_labels, epochs=epochs, batch_size=batch_size,\n",
    "                               validation_data=(X_test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "=== CNN (skip-gram) Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      1.00      0.62         9\n",
      "           1       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.45        20\n",
      "   macro avg       0.23      0.50      0.31        20\n",
      "weighted avg       0.20      0.45      0.28        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\18052\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\18052\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\18052\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# 预测与评估\n",
    "cnn_pred_probs = cnn_skipgram.predict(X_test)\n",
    "cnn_preds = (cnn_pred_probs > 0.5).astype(int).reshape(-1)\n",
    "print(\"=== CNN (skip-gram) Classification Report ===\")\n",
    "print(classification_report(test_labels, cnn_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
